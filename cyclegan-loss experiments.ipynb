{"cells":[{"cell_type":"markdown","metadata":{"id":"v1CUZ0dkOo_F"},"source":["##### Copyright 2019 The TensorFlow Authors."]},{"cell_type":"code","execution_count":94,"metadata":{"cellView":"form","id":"qmkj-80IHxnd","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."]},{"cell_type":"markdown","metadata":{"id":"_xnMOsbqHz61"},"source":["# CycleGAN"]},{"cell_type":"markdown","metadata":{"id":"Ds4o1h4WHz9U"},"source":["<table class=\"tfo-notebook-buttons\" align=\"left\">\n","  <td>\n","    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/generative/cyclegan\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n","  </td>\n","  <td>\n","    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/cyclegan.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n","  </td>\n","  <td>\n","    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/cyclegan.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n","  </td>\n","  <td>\n","    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/generative/cyclegan.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n","  </td>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"ITZuApL56Mny"},"source":["This notebook demonstrates unpaired image to image translation using conditional GAN's, as described in [Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](https://arxiv.org/abs/1703.10593), also known as CycleGAN. The paper proposes a method that can capture the characteristics of one image domain and figure out how these characteristics could be translated into another image domain, all in the absence of any paired training examples. \n","\n","This notebook assumes you are familiar with Pix2Pix, which you can learn about in the [Pix2Pix tutorial](https://www.tensorflow.org/tutorials/generative/pix2pix). The code for CycleGAN is similar, the main difference is an additional loss function, and the use of unpaired training data.\n","\n","CycleGAN uses a cycle consistency loss to enable training without the need for paired data. In other words, it can translate from one domain to another without a one-to-one mapping between the source and target domain. \n","\n","This opens up the possibility to do a lot of interesting tasks like photo-enhancement, image colorization, style transfer, etc. All you need is the source and the target dataset (which is simply a directory of images).\n","\n","![Output Image 1](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/horse2zebra_1.png?raw=1)\n","![Output Image 2](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/horse2zebra_2.png?raw=1)"]},{"cell_type":"markdown","metadata":{"id":"e1_Y75QXJS6h"},"source":["## Set up the input pipeline"]},{"cell_type":"markdown","metadata":{"id":"5fGHWOKPX4ta"},"source":["Install the [tensorflow_examples](https://github.com/tensorflow/examples) package that enables importing of the generator and the discriminator."]},{"cell_type":"code","execution_count":95,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21284,"status":"ok","timestamp":1652241131809,"user":{"displayName":"Andrew Wang","userId":"01366057211587577424"},"user_tz":240},"id":"bJ1ROiQxJ-vY","outputId":"30063cb6-036a-49da-e53b-601a651bcee9","vscode":{"languageId":"python"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/tensorflow/examples.git\n","  Cloning https://github.com/tensorflow/examples.git to c:\\users\\mtapi\\appdata\\local\\temp\\pip-req-build-31qqsnns\n","  Resolved https://github.com/tensorflow/examples.git to commit bf193f680a7d7ea25cd76046c10dac0335696255\n","  Preparing metadata (setup.py): started\n","  Preparing metadata (setup.py): finished with status 'done'\n","Requirement already satisfied: absl-py in c:\\users\\mtapi\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-examples===bf193f680a7d7ea25cd76046c10dac0335696255-) (1.0.0)\n","Requirement already satisfied: six in c:\\users\\mtapi\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-examples===bf193f680a7d7ea25cd76046c10dac0335696255-) (1.16.0)\n","Building wheels for collected packages: tensorflow-examples\n","  Building wheel for tensorflow-examples (setup.py): started\n","  Building wheel for tensorflow-examples (setup.py): finished with status 'done'\n","  Created wheel for tensorflow-examples: filename=tensorflow_examples-bf193f680a7d7ea25cd76046c10dac0335696255_-py3-none-any.whl size=301320 sha256=1d7876035dc9c024516273e80b2caffffe2abb958bb098eff0c727103116bdb9\n","  Stored in directory: C:\\Users\\mtapi\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-_ur0hf4o\\wheels\\72\\5f\\d0\\7fe769eaa229bf20101d11a357eb23c83c481bee2d7f710599\n","Failed to build tensorflow-examples\n","Installing collected packages: tensorflow-examples\n","  Attempting uninstall: tensorflow-examples\n","    Found existing installation: tensorflow-examples e04f01490583a53f220e24990b4cdbc9393ce006-\n","    Uninstalling tensorflow-examples-e04f01490583a53f220e24990b4cdbc9393ce006-:\n","      Successfully uninstalled tensorflow-examples-e04f01490583a53f220e24990b4cdbc9393ce006-\n","  Running setup.py install for tensorflow-examples: started\n","  Running setup.py install for tensorflow-examples: finished with status 'done'\n","Successfully installed tensorflow-examples-022b989af5f154cd6b592ba8e914f9d2c381ddab-\n"]},{"name":"stderr","output_type":"stream","text":["WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n","  Running command git clone --filter=blob:none --quiet https://github.com/tensorflow/examples.git 'C:\\Users\\mtapi\\AppData\\Local\\Temp\\pip-req-build-31qqsnns'\n","  WARNING: Built wheel for tensorflow-examples is invalid: Metadata 1.2 mandates PEP 440 version, but 'bf193f680a7d7ea25cd76046c10dac0335696255-' is not\n","WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n","  DEPRECATION: tensorflow-examples was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368\n","WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n"]}],"source":["!pip install git+https://github.com/tensorflow/examples.git"]},{"cell_type":"code","execution_count":96,"metadata":{"id":"lhSsUx9Nyb3t","vscode":{"languageId":"python"}},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"code","execution_count":97,"metadata":{"id":"9XBfJECyXcWm","vscode":{"languageId":"python"}},"outputs":[],"source":["import glob\n","from PIL import Image"]},{"cell_type":"code","execution_count":98,"metadata":{"id":"YfIk2es3hJEd","vscode":{"languageId":"python"}},"outputs":[],"source":["import tensorflow_datasets as tfds\n","from tensorflow_examples.models.pix2pix import pix2pix\n","\n","import os\n","import time\n","import matplotlib.pyplot as plt\n","from IPython.display import clear_output\n","import numpy as np\n","import pandas as pd\n","\n","AUTOTUNE = tf.data.AUTOTUNE"]},{"cell_type":"markdown","metadata":{"id":"iYn4MdZnKCey"},"source":["## Input Pipeline\n","\n","This tutorial trains a model to translate from images of horses, to images of zebras. You can find this dataset and similar ones [here](https://www.tensorflow.org/datasets/catalog/cycle_gan). \n","\n","As mentioned in the [paper](https://arxiv.org/abs/1703.10593), apply random jittering and mirroring to the training dataset. These are some of the image augmentation techniques that avoids overfitting.\n","\n","This is similar to what was done in [pix2pix](https://www.tensorflow.org/tutorials/generative/pix2pix#load_the_dataset)\n","\n","* In random jittering, the image is resized to `286 x 286` and then randomly cropped to `256 x 256`.\n","* In random mirroring, the image is randomly flipped horizontally i.e left to right."]},{"cell_type":"code","execution_count":99,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23597,"status":"ok","timestamp":1652241160456,"user":{"displayName":"Andrew Wang","userId":"01366057211587577424"},"user_tz":240},"id":"nvYb-MbJXkMP","outputId":"abc90bfd-49c5-4444-a676-a0743cf93da6","vscode":{"languageId":"python"}},"outputs":[],"source":["#from google.colab import drive\n","#drive_dir = '/content/drive'\n","#drive.mount(drive_dir, force_remount=True) "]},{"cell_type":"code","execution_count":100,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":140,"status":"ok","timestamp":1652241248873,"user":{"displayName":"Andrew Wang","userId":"01366057211587577424"},"user_tz":240},"id":"_e5J14qbXl71","outputId":"3411b52d-a796-4db7-db15-c0b8abca1be8","vscode":{"languageId":"python"}},"outputs":[{"name":"stdout","output_type":"stream","text":[".//Landscape_Images .//camo_subset_raw\n"]}],"source":["main_dir = f'./'\n","landscape_dir = 'Landscape_Images' \n","camo_dir = 'camo_subset_raw'\n","\n","landscape_path = f\"{main_dir}/{landscape_dir}\"\n","camo_path = f\"{main_dir}/{camo_dir}\"\n","\n","print(landscape_path, camo_path)"]},{"cell_type":"code","execution_count":101,"metadata":{"id":"5qh6_EmMlOPM","vscode":{"languageId":"python"}},"outputs":[],"source":["# file_path = landscape_path\n","\n","# images_dict = {os.path.splitext(os.path.basename(x))[0]: x\n","#                     for x in glob.glob(os.path.join(file_path, '', '*.jpg'))}"]},{"cell_type":"code","execution_count":102,"metadata":{"id":"5khTqu7ulT1z","vscode":{"languageId":"python"}},"outputs":[],"source":["# df = pd.DataFrame(images_dict.items())\n","# #rename columns for clarity\n","# df = df.rename(columns={df.columns[0]: \"image_id\"})\n","# df = df.rename(columns={df.columns[1]: \"image_path\"})\n","\n","# df[\"image\"] = df['image_path'].map(lambda x: np.asarray(Image.open(x).resize((256, 256))))\n","\n","# imgs = df[\"image\"]\n","# inputs = np.array(imgs)/(255.0) # normalization\n","\n","# interim_inputs = [j for i, j in enumerate(inputs)]"]},{"cell_type":"code","execution_count":103,"metadata":{"id":"wmIlElU9ladf","vscode":{"languageId":"python"}},"outputs":[],"source":["# print([i.shape for i in interim_inputs])"]},{"cell_type":"code","execution_count":104,"metadata":{"id":"iuGVPOo7Cce0","vscode":{"languageId":"python"}},"outputs":[],"source":["# dataset, metadata = tfds.load('cycle_gan/horse2zebra',\n","#                               with_info=True, as_supervised=True)\n","\n","# train_horses, train_zebras = dataset['trainA'], dataset['trainB']\n","# test_horses, test_zebras = dataset['testA'], dataset['testB']\n","def data_preprocessor(file_path, limit=None):\n","\n","    #create dataframe to store image paths and images:\n","\n","    images_dict = {os.path.splitext(os.path.basename(x))[0]: x\n","                    for x in glob.glob(os.path.join(file_path, '', '*.jpg'))}\n","\n","    df = pd.DataFrame(images_dict.items())\n","    #rename columns for clarity\n","    df = df.rename(columns={df.columns[0]: \"image_id\"})\n","    df = df.rename(columns={df.columns[1]: \"image_path\"})\n","    if limit:\n","      df = df[:limit]\n","    df[\"image\"] = df['image_path'].map(lambda x: np.asarray(Image.open(x).resize((256, 256))))\n","\n","    imgs = df[\"image\"]\n","    inputs = np.array(imgs)/(255.0) # normalization\n","\n","    interim_inputs = [j for i, j in enumerate(inputs)]\n","    inp_reshape = tf.reshape(interim_inputs, (-1, 256, 256, 3))\n","    train_imgs = np.asarray(inp_reshape, dtype= np.float32)\n","\n","    # train_imgs = tf.random.shuffle(final_inputs)\n","\n","    og_images = []\n","    for tensor in train_imgs:\n","        og_images.append(tensor)\n","\n","    # Apply data augmentation to populate some data\n","    # With data augmentation to prevent overfitting\n","    \"\"\"\n","    lst_saturated = []\n","    for i in range(len(train_imgs)):\n","        saturation_played_1_3 = tf.image.adjust_saturation(train_imgs[i], 1.3)\n","        saturation_played_1_6 = tf.image.adjust_saturation(train_imgs[i], 1.6)\n","        saturation_played_1_9 = tf.image.adjust_saturation(train_imgs[i], 1.9)\n","        lst_saturated.append(saturation_played_1_3)\n","        lst_saturated.append(saturation_played_1_6)\n","        lst_saturated.append(saturation_played_1_9)\n","\n","    res_list = [y for x in [og_images, lst_saturated] for y in x]\n","    \"\"\"\n","\n","    tensor_converted_images = tf.convert_to_tensor(og_images)\n","    image_dataset = tf.data.Dataset.from_tensor_slices(tensor_converted_images)\n","\n","    ds_size = tf.data.experimental.cardinality(image_dataset)\n","    train_split=0.8\n","    test_split=0.2\n","    shuffle_size=296\n","\n","    Shuffle=True\n","    if Shuffle:\n","    # Specify seed to always have the same split distribution between runs\n","        ds = image_dataset.shuffle(shuffle_size, seed=12)\n","\n","    train_size = int(np.ceil(train_split * int(ds_size)))\n","    test_size = int(np.ceil(test_split * int(ds_size)))\n","\n","    train_ds = ds.take(train_size)   \n","    test_ds = ds.take(test_size)\n","\n","    train_size_lst = []\n","    for img in train_ds:\n","        train_size_lst.append(img)\n","\n","    train_imgs_arrays = []\n","    for tensor_images in train_size_lst:\n","        array_img = np.asarray(tensor_images)\n","        train_imgs_arrays.append(array_img)\n","\n","    # dark_skin_sample = train_imgs_arrays[20]\n","\n","    return train_ds, test_ds # , dark_skin_sample\n"]},{"cell_type":"code","execution_count":105,"metadata":{"id":"gLrLQquce7eu","vscode":{"languageId":"python"}},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/util/dispatch.py?line=1080'>1081</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/util/dispatch.py?line=1081'>1082</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/util/dispatch.py?line=1082'>1083</a>\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/util/dispatch.py?line=1083'>1084</a>\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/util/dispatch.py?line=1084'>1085</a>\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\array_ops.py:194\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(tensor, shape, name)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/ops/array_ops.py?line=60'>61</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Reshapes a tensor.\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/ops/array_ops.py?line=61'>62</a>\u001b[0m \n\u001b[0;32m     <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/ops/array_ops.py?line=62'>63</a>\u001b[0m \u001b[39mGiven `tensor`, this operation returns a new `tf.Tensor` that has the same\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/ops/array_ops.py?line=191'>192</a>\u001b[0m \u001b[39m  A `Tensor`. Has the same type as `tensor`.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/ops/array_ops.py?line=192'>193</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/ops/array_ops.py?line=193'>194</a>\u001b[0m result \u001b[39m=\u001b[39m gen_array_ops\u001b[39m.\u001b[39;49mreshape(tensor, shape, name)\n\u001b[0;32m    <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/ops/array_ops.py?line=194'>195</a>\u001b[0m tensor_util\u001b[39m.\u001b[39mmaybe_set_static_shape(result, shape)\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:8540\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(tensor, shape, name)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/ops/gen_array_ops.py?line=8538'>8539</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/ops/gen_array_ops.py?line=8539'>8540</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m reshape_eager_fallback(\n\u001b[0;32m   <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/ops/gen_array_ops.py?line=8540'>8541</a>\u001b[0m       tensor, shape, name\u001b[39m=\u001b[39;49mname, ctx\u001b[39m=\u001b[39;49m_ctx)\n\u001b[0;32m   <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/ops/gen_array_ops.py?line=8541'>8542</a>\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_SymbolicException:\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:8561\u001b[0m, in \u001b[0;36mreshape_eager_fallback\u001b[1;34m(tensor, shape, name, ctx)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/ops/gen_array_ops.py?line=8559'>8560</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreshape_eager_fallback\u001b[39m(tensor, shape, name, ctx):\n\u001b[1;32m-> <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/ops/gen_array_ops.py?line=8560'>8561</a>\u001b[0m   _attr_T, (tensor,) \u001b[39m=\u001b[39m _execute\u001b[39m.\u001b[39;49margs_to_matching_eager([tensor], ctx, [])\n\u001b[0;32m   <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/ops/gen_array_ops.py?line=8561'>8562</a>\u001b[0m   _attr_Tshape, (shape,) \u001b[39m=\u001b[39m _execute\u001b[39m.\u001b[39margs_to_matching_eager([shape], ctx, [_dtypes\u001b[39m.\u001b[39mint32, _dtypes\u001b[39m.\u001b[39mint64, ], _dtypes\u001b[39m.\u001b[39mint32)\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:264\u001b[0m, in \u001b[0;36margs_to_matching_eager\u001b[1;34m(l, ctx, allowed_dtypes, default_dtype)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/eager/execute.py?line=262'>263</a>\u001b[0m \u001b[39mif\u001b[39;00m tensor \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/eager/execute.py?line=263'>264</a>\u001b[0m   tensor \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49mconvert_to_tensor(\n\u001b[0;32m    <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/eager/execute.py?line=264'>265</a>\u001b[0m       t, dtype, preferred_dtype\u001b[39m=\u001b[39;49mdefault_dtype, ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/eager/execute.py?line=266'>267</a>\u001b[0m ret\u001b[39m.\u001b[39mappend(tensor)\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\profiler\\trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/profiler/trace.py?line=181'>182</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/profiler/trace.py?line=182'>183</a>\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\framework\\ops.py:1695\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/framework/ops.py?line=1693'>1694</a>\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/framework/ops.py?line=1694'>1695</a>\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[0;32m   <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/framework/ops.py?line=1696'>1697</a>\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\framework\\constant_op.py:343\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/framework/constant_op.py?line=341'>342</a>\u001b[0m _ \u001b[39m=\u001b[39m as_ref\n\u001b[1;32m--> <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/framework/constant_op.py?line=342'>343</a>\u001b[0m \u001b[39mreturn\u001b[39;00m constant(v, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname)\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\framework\\constant_op.py:267\u001b[0m, in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/framework/constant_op.py?line=171'>172</a>\u001b[0m \u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/framework/constant_op.py?line=172'>173</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/framework/constant_op.py?line=173'>174</a>\u001b[0m \u001b[39mNote: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/framework/constant_op.py?line=264'>265</a>\u001b[0m \u001b[39m  ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/framework/constant_op.py?line=265'>266</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/framework/constant_op.py?line=266'>267</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/framework/constant_op.py?line=267'>268</a>\u001b[0m                       allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\framework\\constant_op.py:279\u001b[0m, in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/framework/constant_op.py?line=277'>278</a>\u001b[0m       \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m--> <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/framework/constant_op.py?line=278'>279</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m    <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/framework/constant_op.py?line=280'>281</a>\u001b[0m g \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mget_default_graph()\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\framework\\constant_op.py:304\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/framework/constant_op.py?line=302'>303</a>\u001b[0m \u001b[39m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/framework/constant_op.py?line=303'>304</a>\u001b[0m t \u001b[39m=\u001b[39m convert_to_eager_tensor(value, ctx, dtype)\n\u001b[0;32m    <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/framework/constant_op.py?line=304'>305</a>\u001b[0m \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/framework/constant_op.py?line=100'>101</a>\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/framework/constant_op.py?line=101'>102</a>\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n","\u001b[1;31mValueError\u001b[0m: Can't convert non-rectangular Python sequence to Tensor.","\nDuring handling of the above exception, another exception occurred:\n","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32mc:\\Users\\mtapi\\OneDrive\\Documents\\Brown\\cs2470\\Final_Project\\g_drive\\cyclegan-loss experiments.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mtapi/OneDrive/Documents/Brown/cs2470/Final_Project/g_drive/cyclegan-loss%20experiments.ipynb#ch0000018?line=0'>1</a>\u001b[0m LIMIT \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mtapi/OneDrive/Documents/Brown/cs2470/Final_Project/g_drive/cyclegan-loss%20experiments.ipynb#ch0000018?line=2'>3</a>\u001b[0m train_landscape, test_landscape \u001b[39m=\u001b[39m data_preprocessor(landscape_path, limit\u001b[39m=\u001b[39;49mLIMIT)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mtapi/OneDrive/Documents/Brown/cs2470/Final_Project/g_drive/cyclegan-loss%20experiments.ipynb#ch0000018?line=3'>4</a>\u001b[0m train_camo, test_camo \u001b[39m=\u001b[39m data_preprocessor(camo_path, limit\u001b[39m=\u001b[39mLIMIT)\n","\u001b[1;32mc:\\Users\\mtapi\\OneDrive\\Documents\\Brown\\cs2470\\Final_Project\\g_drive\\cyclegan-loss experiments.ipynb Cell 18'\u001b[0m in \u001b[0;36mdata_preprocessor\u001b[1;34m(file_path, limit)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mtapi/OneDrive/Documents/Brown/cs2470/Final_Project/g_drive/cyclegan-loss%20experiments.ipynb#ch0000017?line=21'>22</a>\u001b[0m inputs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(imgs)\u001b[39m/\u001b[39m(\u001b[39m255.0\u001b[39m) \u001b[39m# normalization\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mtapi/OneDrive/Documents/Brown/cs2470/Final_Project/g_drive/cyclegan-loss%20experiments.ipynb#ch0000017?line=23'>24</a>\u001b[0m interim_inputs \u001b[39m=\u001b[39m [j \u001b[39mfor\u001b[39;00m i, j \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(inputs)]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mtapi/OneDrive/Documents/Brown/cs2470/Final_Project/g_drive/cyclegan-loss%20experiments.ipynb#ch0000017?line=24'>25</a>\u001b[0m inp_reshape \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mreshape(interim_inputs, (\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m256\u001b[39;49m, \u001b[39m256\u001b[39;49m, \u001b[39m3\u001b[39;49m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mtapi/OneDrive/Documents/Brown/cs2470/Final_Project/g_drive/cyclegan-loss%20experiments.ipynb#ch0000017?line=25'>26</a>\u001b[0m train_imgs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(inp_reshape, dtype\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mtapi/OneDrive/Documents/Brown/cs2470/Final_Project/g_drive/cyclegan-loss%20experiments.ipynb#ch0000017?line=27'>28</a>\u001b[0m \u001b[39m# train_imgs = tf.random.shuffle(final_inputs)\u001b[39;00m\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/util/traceback_utils.py?line=147'>148</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/util/traceback_utils.py?line=148'>149</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/util/traceback_utils.py?line=149'>150</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/util/dispatch.py?line=1079'>1080</a>\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/util/dispatch.py?line=1080'>1081</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/util/dispatch.py?line=1081'>1082</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/util/dispatch.py?line=1082'>1083</a>\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/util/dispatch.py?line=1083'>1084</a>\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/util/dispatch.py?line=1084'>1085</a>\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/mtapi/AppData/Roaming/Python/Python310/site-packages/tensorflow/python/util/dispatch.py?line=1085'>1086</a>\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["LIMIT = 300\n","\n","train_landscape, test_landscape = data_preprocessor(landscape_path, limit=LIMIT)\n","train_camo, test_camo = data_preprocessor(camo_path, limit=LIMIT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2CbTEt448b4R","vscode":{"languageId":"python"}},"outputs":[],"source":["BUFFER_SIZE = 1000\n","BATCH_SIZE = 1\n","IMG_WIDTH = 256\n","IMG_HEIGHT = 256"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yn3IwqhiIszt","vscode":{"languageId":"python"}},"outputs":[],"source":["def random_crop(image):\n","  cropped_image = tf.image.random_crop(\n","      image, size=[IMG_HEIGHT, IMG_WIDTH, 3])\n","\n","  return cropped_image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"muhR2cgbLKWW","vscode":{"languageId":"python"}},"outputs":[],"source":["# normalizing the images to [-1, 1]\n","def normalize(image):\n","  image = tf.cast(image, tf.float32)\n","  image = (image / 127.5) - 1\n","  return image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fVQOjcPVLrUc","vscode":{"languageId":"python"}},"outputs":[],"source":["def random_jitter(image):\n","  # resizing to 286 x 286 x 3\n","  image = tf.image.resize(image, [286, 286],\n","                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n","\n","  # randomly cropping to 256 x 256 x 3\n","  image = random_crop(image)\n","\n","  # random mirroring\n","  image = tf.image.random_flip_left_right(image)\n","\n","  return image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tyaP4hLJ8b4W","vscode":{"languageId":"python"}},"outputs":[],"source":["def preprocess_image_train(image, label):\n","  image = random_jitter(image)\n","  image = normalize(image)\n","  return image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VB3Z6D_zKSru","vscode":{"languageId":"python"}},"outputs":[],"source":["def preprocess_image_test(image, label):\n","  image = normalize(image)\n","  return image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RsajGXxd5JkZ","vscode":{"languageId":"python"}},"outputs":[],"source":["# train_horses = train_horses.cache().map(\n","#     preprocess_image_train, num_parallel_calls=AUTOTUNE).shuffle(\n","#     BUFFER_SIZE).batch(BATCH_SIZE)\n","\n","# train_zebras = train_zebras.cache().map(\n","#     preprocess_image_train, num_parallel_calls=AUTOTUNE).shuffle(\n","#     BUFFER_SIZE).batch(BATCH_SIZE)\n","\n","# test_horses = test_horses.map(\n","#     preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(\n","#     BUFFER_SIZE).batch(BATCH_SIZE)\n","\n","# test_zebras = test_zebras.map(\n","#     preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(\n","#     BUFFER_SIZE).batch(BATCH_SIZE)\n","\n","train_landscapes = train_landscape.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n","test_landscapes = test_landscape.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n","train_camos = train_camo.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n","test_camos = test_camo.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"]},{"cell_type":"markdown","metadata":{"id":"q6pEv-1cO8mL"},"source":["## Sample Landscape and Camo for benchmarks/vis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e3MhJ3zVLPan","vscode":{"languageId":"python"}},"outputs":[],"source":["sample_landscape = next(iter(train_landscapes))\n","sample_camo = next(iter(train_camos))"]},{"cell_type":"markdown","metadata":{"id":"u1IubivaO_hZ"},"source":["## Template for displaying images (generated or otherwise)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":233},"executionInfo":{"elapsed":920,"status":"ok","timestamp":1652241414637,"user":{"displayName":"Andrew Wang","userId":"01366057211587577424"},"user_tz":240},"id":"4pOYjMk_KfIB","outputId":"57d66a3c-a636-4fef-83bf-7f8dfbee37a3","vscode":{"languageId":"python"}},"outputs":[],"source":["plt.subplot(121)\n","plt.title('Landscape')\n","plt.imshow(sample_landscape[0] * 0.5 + 0.5)\n","\n","plt.subplot(122)\n","plt.title('Landscape with random jitter')\n","plt.imshow(random_jitter(sample_landscape[0]) * 0.5 + 0.5)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":233},"executionInfo":{"elapsed":651,"status":"ok","timestamp":1652241415275,"user":{"displayName":"Andrew Wang","userId":"01366057211587577424"},"user_tz":240},"id":"0KJyB9ENLb2y","outputId":"4abf1039-5eac-4c65-f9bf-ebe3db8449ca","vscode":{"languageId":"python"}},"outputs":[],"source":["plt.subplot(121)\n","plt.title('Camo')\n","plt.imshow(sample_camo[0] * 0.5 + 0.5)\n","\n","plt.subplot(122)\n","plt.title('Camo with random jitter')\n","plt.imshow(random_jitter(sample_camo[0]) * 0.5 + 0.5)"]},{"cell_type":"markdown","metadata":{"id":"hvX8sKsfMaio"},"source":["## Import and reuse the Pix2Pix models"]},{"cell_type":"markdown","metadata":{"id":"cGrL73uCd-_M"},"source":["Import the generator and the discriminator used in [Pix2Pix](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py) via the installed [tensorflow_examples](https://github.com/tensorflow/examples) package.\n","\n","The model architecture used in this tutorial is very similar to what was used in [pix2pix](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py). Some of the differences are:\n","\n","* Cyclegan uses [instance normalization](https://arxiv.org/abs/1607.08022) instead of [batch normalization](https://arxiv.org/abs/1502.03167).\n","* The [CycleGAN paper](https://arxiv.org/abs/1703.10593) uses a modified `resnet` based generator. This tutorial is using a modified `unet` generator for simplicity.\n","\n","There are 2 generators (G and F) and 2 discriminators (X and Y) being trained here. \n","\n","* Generator `G` learns to transform image `X` to image `Y`. $(G: X -> Y)$\n","* Generator `F` learns to transform image `Y` to image `X`. $(F: Y -> X)$\n","* Discriminator `D_X` learns to differentiate between image `X` and generated image `X` (`F(Y)`).\n","* Discriminator `D_Y` learns to differentiate between image `Y` and generated image `Y` (`G(X)`).\n","\n","![Cyclegan model](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/cyclegan_model.png?raw=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8ju9Wyw87MRW","vscode":{"languageId":"python"}},"outputs":[],"source":["OUTPUT_CHANNELS = 3\n","\n","generator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n","generator_f = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n","\n","discriminator_x = pix2pix.discriminator(norm_type='instancenorm', target=False)\n","discriminator_y = pix2pix.discriminator(norm_type='instancenorm', target=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":534},"executionInfo":{"elapsed":5642,"status":"ok","timestamp":1652241426489,"user":{"displayName":"Andrew Wang","userId":"01366057211587577424"},"user_tz":240},"id":"wDaGZ3WpZUyw","outputId":"e57930ab-d02d-47c3-b31c-632a56500bb3","vscode":{"languageId":"python"}},"outputs":[],"source":["to_camo = generator_g(sample_landscape)\n","to_landscape = generator_f(sample_camo)\n","plt.figure(figsize=(8, 8))\n","contrast = 8\n","\n","imgs = [sample_landscape, to_camo, sample_camo, to_landscape]\n","title = ['Landscape', 'To Camo', 'Camo', 'To Landscape']\n","\n","for i in range(len(imgs)):\n","  plt.subplot(2, 2, i+1)\n","  plt.title(title[i])\n","  if i % 2 == 0:\n","    plt.imshow(imgs[i][0] * 0.5 + 0.5)\n","  else:\n","    plt.imshow(imgs[i][0] * 0.5 * contrast + 0.5)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":267},"executionInfo":{"elapsed":1830,"status":"ok","timestamp":1652241428273,"user":{"displayName":"Andrew Wang","userId":"01366057211587577424"},"user_tz":240},"id":"O5MhJmxyZiy9","outputId":"09fd8a17-b45b-4cd0-ac36-fea6ef97fbff","vscode":{"languageId":"python"}},"outputs":[],"source":["plt.figure(figsize=(8, 8))\n","\n","plt.subplot(121)\n","plt.title('Is a real camo?')\n","plt.imshow(discriminator_y(sample_camo)[0, ..., -1], cmap='RdBu_r')\n","\n","plt.subplot(122)\n","plt.title('Is a real landscape?')\n","plt.imshow(discriminator_x(sample_landscape)[0, ..., -1], cmap='RdBu_r')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"0FMYgY_mPfTi"},"source":["## Loss functions"]},{"cell_type":"markdown","metadata":{"id":"JRqt02lupRn8"},"source":["In CycleGAN, there is no paired data to train on, hence there is no guarantee that the input `x` and the target `y` pair are meaningful during training. Thus in order to enforce that the network learns the correct mapping, the authors propose the cycle consistency loss.\n","\n","The discriminator loss and the generator loss are similar to the ones used in [pix2pix](https://www.tensorflow.org/tutorials/generative/pix2pix#build_the_generator)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cyhxTuvJyIHV","vscode":{"languageId":"python"}},"outputs":[],"source":["LAMBDA = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q1Xbz5OaLj5C","vscode":{"languageId":"python"}},"outputs":[],"source":["loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wkMNfBWlT-PV","vscode":{"languageId":"python"}},"outputs":[],"source":["def discriminator_loss(real, generated):\n","  real_loss = loss_obj(tf.ones_like(real), real)\n","\n","  generated_loss = loss_obj(tf.zeros_like(generated), generated)\n","\n","  total_disc_loss = real_loss + generated_loss\n","\n","  return total_disc_loss * 0.5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"90BIcCKcDMxz","vscode":{"languageId":"python"}},"outputs":[],"source":["def generator_loss(generated):\n","  return loss_obj(tf.ones_like(generated), generated)"]},{"cell_type":"markdown","metadata":{"id":"5iIWQzVF7f9e"},"source":["Cycle consistency means the result should be close to the original input. For example, if one translates a sentence from English to French, and then translates it back from French to English, then the resulting sentence should be the same as the  original sentence.\n","\n","In cycle consistency loss, \n","\n","* Image $X$ is passed via generator $G$ that yields generated image $\\hat{Y}$.\n","* Generated image $\\hat{Y}$ is passed via generator $F$ that yields cycled image $\\hat{X}$.\n","* Mean absolute error is calculated between $X$ and $\\hat{X}$.\n","\n","$$forward\\ cycle\\ consistency\\ loss: X -> G(X) -> F(G(X)) \\sim \\hat{X}$$\n","\n","$$backward\\ cycle\\ consistency\\ loss: Y -> F(Y) -> G(F(Y)) \\sim \\hat{Y}$$\n","\n","\n","![Cycle loss](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/cycle_loss.png?raw=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NMpVGj_sW6Vo","vscode":{"languageId":"python"}},"outputs":[],"source":["def calc_cycle_loss(real_image, cycled_image, loss_weight=10):\n","  loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n","  \n","  return loss_weight * loss1"]},{"cell_type":"markdown","metadata":{"id":"U-tJL-fX0Mq7"},"source":["As shown above, generator $G$ is responsible for translating image $X$ to image $Y$. Identity loss says that, if you fed image $Y$ to generator $G$, it should yield the real image $Y$ or something close to image $Y$.\n","\n","If you run the zebra-to-horse model on a horse or the horse-to-zebra model on a zebra, it should not modify the image much since the image already contains the target class.\n","\n","$$Identity\\ loss = |G(Y) - Y| + |F(X) - X|$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"05ywEH680Aud","vscode":{"languageId":"python"}},"outputs":[],"source":["def identity_loss(real_image, same_image, loss_wieght=10):\n","  loss = tf.reduce_mean(tf.abs(real_image - same_image))\n","  return loss_wieght * 0.5 * loss"]},{"cell_type":"markdown","metadata":{"id":"G-vjRM7IffTT"},"source":["Initialize the optimizers for all the generators and the discriminators."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iWCn_PVdEJZ7","vscode":{"languageId":"python"}},"outputs":[],"source":["generator_g_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n","generator_f_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n","\n","discriminator_x_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n","discriminator_y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"]},{"cell_type":"markdown","metadata":{"id":"aKUZnDiqQrAh"},"source":["## Checkpoints"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WJnftd5sQsv6","vscode":{"languageId":"python"}},"outputs":[],"source":["checkpoint_path = \"./checkpoints/train\"\n","\n","ckpt = tf.train.Checkpoint(generator_g=generator_g,\n","                           generator_f=generator_f,\n","                           discriminator_x=discriminator_x,\n","                           discriminator_y=discriminator_y,\n","                           generator_g_optimizer=generator_g_optimizer,\n","                           generator_f_optimizer=generator_f_optimizer,\n","                           discriminator_x_optimizer=discriminator_x_optimizer,\n","                           discriminator_y_optimizer=discriminator_y_optimizer)\n","\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n","\n","# if a checkpoint exists, restore the latest checkpoint.\n","if ckpt_manager.latest_checkpoint:\n","  ckpt.restore(ckpt_manager.latest_checkpoint)\n","  print ('Latest checkpoint restored!!')"]},{"cell_type":"markdown","metadata":{"id":"Rw1fkAczTQYh"},"source":["## Training\n","\n","Note: This example model is trained for fewer epochs (40) than the paper (200) to keep training time reasonable for this tutorial. Predictions may be less accurate. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NS2GWywBbAWo","vscode":{"languageId":"python"}},"outputs":[],"source":["EPOCHS = 40"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RmdVsmvhPxyy","vscode":{"languageId":"python"}},"outputs":[],"source":["def generate_images(model, test_input):\n","  prediction = model(test_input)\n","    \n","  plt.figure(figsize=(12, 12))\n","\n","  display_list = [test_input[0], prediction[0]]\n","  title = ['Input Image', 'Predicted Image']\n","\n","  for i in range(2):\n","    plt.subplot(1, 2, i+1)\n","    plt.title(title[i])\n","    # getting the pixel values between [0, 1] to plot it.\n","    plt.imshow(display_list[i] * 0.5 + 0.5)\n","    plt.axis('off')\n","  plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2TIqlpMAhjaS","vscode":{"languageId":"python"}},"outputs":[],"source":["def generate_patched_landscape(model, landscape):\n","    fake_camo = generator_g(landscape)\n","    patched_landscape = cutout_and_replace(sample_landscape, fake_camo, cutout_size=64)\n","\n","    plt.figure(figsize=(12, 12))\n","\n","    display_list = [landscape[0], patched_landscape[0]]\n","    title = ['Input Landscape', 'Patched Landscape']\n","\n","    for i in range(2):\n","        plt.subplot(1, 2, i+1)\n","        plt.title(title[i])\n","        # getting the pixel values between [0, 1] to plot it.\n","        plt.imshow(display_list[i] * 0.5 + 0.5)\n","        plt.axis('off')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tLzQ4DQkVFZV","vscode":{"languageId":"python"}},"outputs":[],"source":["def save_generated_results(model, landscape, filename):\n","    fake_camo = model(landscape)\n","    patched_landscape = cutout_and_replace(landscape, fake_camo, cutout_size=64)\n","\n","    plt.figure(figsize=(12, 12))\n","\n","    display_list = [fake_camo[0], patched_landscape[0]]\n","    title = ['Fake Camo', 'Patched Landscape']\n","\n","    for i in range(2):\n","        plt.subplot(1, 2, i+1)\n","        plt.title(title[i])\n","        # getting the pixel values between [0, 1] to plot it.\n","        plt.imshow(display_list[i] * 0.5 + 0.5)\n","        plt.axis('off')\n","    plt.savefig(filename)"]},{"cell_type":"markdown","metadata":{"id":"kE47ERn5fyLC"},"source":["Even though the training loop looks complicated, it consists of four basic steps:\n","\n","* Get the predictions.\n","* Calculate the loss.\n","* Calculate the gradients using backpropagation.\n","* Apply the gradients to the optimizer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LTttRDVn9EYT","vscode":{"languageId":"python"}},"outputs":[],"source":["def cutout_and_replace(orig_landscape, camo, cutout_size=16):\n","    H, W = orig_landscape.shape[1:3]\n","    assert cutout_size <= H//2\n","\n","    # Determining square location\n","    H_start = int(H//2)\n","    x_start = np.random.randint(0, W - cutout_size)\n","    y_start = np.random.randint(H_start, H - cutout_size)\n","\n","    # Square indices\n","    xs = list(range(x_start, x_start+cutout_size))\n","    ys = list(range(y_start, y_start+cutout_size))\n","    list_idxs = np.dstack(np.meshgrid(ys, xs)).reshape(-1, 2)\n","\n","    # Resizing camo patch\n","    resized_camo_patch = tf.image.resize(camo, [cutout_size, cutout_size])\n","    resized_camo_patch = tf.transpose(resized_camo_patch, perm=[0, 2, 1, 3]) # Transpose image\n","    list_camo_pix = tf.reshape(resized_camo_patch, [cutout_size**2, 3])\n","\n","    # Replacing with resized camo\n","    updated_landscape = tf.tensor_scatter_nd_update(orig_landscape[0], list_idxs, list_camo_pix)\n","    print(updated_landscape.shape)\n","\n","    return tf.expand_dims(updated_landscape, 0) # Convert back to (1, H, W, 3)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":368},"executionInfo":{"elapsed":640,"status":"ok","timestamp":1652241428374,"user":{"displayName":"Andrew Wang","userId":"01366057211587577424"},"user_tz":240},"id":"bd4xgtEEjuby","outputId":"cead0259-04c9-44a1-de05-b3e671588255","vscode":{"languageId":"python"}},"outputs":[],"source":["generate_patched_landscape(generator_g, sample_landscape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KBKUV2sKXDbY","vscode":{"languageId":"python"}},"outputs":[],"source":["@tf.function\n","def train_step(real_x, real_y, reconst_weight=10, ls_disc_weight=1):\n","  # y - camo\n","  # x - landscape\n","  # persistent is set to True because the tape is used more than\n","  # once to calculate the gradients.\n","  with tf.GradientTape(persistent=True) as tape:\n","    # Generator G translates X -> Y\n","    # Generator F translates Y -> X.\n","    \n","    fake_y = generator_g(real_x, training=True) # generated camouflage\n","    # cycled_x = generator_f(fake_y, training=True)\n","\n","    # fake_x = generator_f(real_y, training=True)\n","    fake_x = cutout_and_replace(real_x, fake_y, cutout_size=64) # cutout/replaced landscape\n","    cycled_y = generator_g(fake_x, training=True) # Might not be necessary for camo GAN; how close does the generator get to producing the same camouflage from landscapes patched by camouflage?\n","\n","    # same_x and same_y are used for identity loss.\n","    # same_x = generator_f(real_x, training=True)\n","    same_y = generator_g(real_y, training=True) # Can generator G produce the same camouflage given itself? (identity)\n","\n","    disc_real_x = discriminator_x(real_x, training=True) # Is real landscape a landscape?\n","    disc_real_y = discriminator_y(real_y, training=True) # Is real camouflage a camouflage?\n","\n","    disc_fake_x = discriminator_x(fake_x, training=True) # Tests: is cutout/replaced landscape a landscape?\n","    disc_fake_y = discriminator_y(fake_y, training=True) # Tests: is generated camouflage a camouflage?\n","\n","    # calculate the loss\n","    gen_g_loss = generator_loss(disc_fake_y)+ ls_disc_weight*generator_loss(disc_fake_x) # Want to fool both discriminators \n","    # gen_f_loss = generator_loss(disc_fake_x)\n","    \n","    # total_cycle_loss = calc_cycle_loss(real_x, cycled_x, reconst_weight) + calc_cycle_loss(real_y, cycled_y, reconst_weight)\n","    total_cycle_loss = calc_cycle_loss(real_y, cycled_y, reconst_weight)\n","    \n","    # Total generator loss = adversarial loss + cycle loss\n","    total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_loss(real_y, same_y, reconst_weight)\n","    # total_gen_f_loss = gen_f_loss + total_cycle_loss + identity_loss(real_x, same_x, reconst_weight)\n","\n","    disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)\n","    disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n","  \n","  # Calculate the gradients for generator and discriminator\n","  generator_g_gradients = tape.gradient(total_gen_g_loss, \n","                                        generator_g.trainable_variables)\n","  # generator_f_gradients = tape.gradient(total_gen_f_loss, \n","  #                                       generator_f.trainable_variables)\n","  \n","  discriminator_x_gradients = tape.gradient(disc_x_loss, \n","                                            discriminator_x.trainable_variables)\n","  discriminator_y_gradients = tape.gradient(disc_y_loss, \n","                                            discriminator_y.trainable_variables)\n","  \n","  # Apply the gradients to the optimizer\n","  generator_g_optimizer.apply_gradients(zip(generator_g_gradients, \n","                                            generator_g.trainable_variables))\n","\n","  # generator_f_optimizer.apply_gradients(zip(generator_f_gradients, \n","  #                                           generator_f.trainable_variables))\n","  \n","  discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients,\n","                                                discriminator_x.trainable_variables))\n","  \n","  discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients,\n","                                                discriminator_y.trainable_variables))"]},{"cell_type":"markdown","metadata":{"id":"SugVyNHaQiZ6"},"source":["## File path to save images:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":820,"status":"ok","timestamp":1652241930748,"user":{"displayName":"Andrew Wang","userId":"01366057211587577424"},"user_tz":240},"id":"mM-c7oP0QkAz","outputId":"9ef228f1-ce7a-4481-a625-464b7c893321","vscode":{"languageId":"python"}},"outputs":[],"source":["image_path = f'{main_dir}/images'\n","print(image_path)"]},{"cell_type":"markdown","metadata":{"id":"7TRUW1Z-f_ko"},"source":["## Patched-Camo GAN:"]},{"cell_type":"markdown","metadata":{"id":"fj1o5QmeorKr"},"source":["reconst_weight=0.5, ls_disc_weight=1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1MawVoMRB8-M-sMKEzNhSbSL5GyX1kgoq"},"executionInfo":{"elapsed":25302,"status":"ok","timestamp":1652243123027,"user":{"displayName":"Andrew Wang","userId":"01366057211587577424"},"user_tz":240},"id":"q4pkTBANgBeP","outputId":"57dd1f6e-e5b8-48a5-910e-a987974d1aab","vscode":{"languageId":"python"}},"outputs":[],"source":["reconst_weight=0.5\n","ls_disc_weight=1\n","\n","for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  n = 0\n","  for image_x, image_y in tf.data.Dataset.zip((train_landscapes, train_camos)):\n","    train_step(image_x, image_y, reconst_weight=reconst_weight, ls_disc_weight=ls_disc_weight)\n","    if n % 10 == 0:\n","      print ('.', end='')\n","    n += 1\n","\n","  clear_output(wait=True)\n","  # Using a consistent image (sample_horse) so that the progress of the model\n","  # is clearly visible.\n","  generate_images(generator_g, sample_landscape)\n","  generate_patched_landscape(generator_g, sample_landscape)\n","\n","\n","  if (epoch + 1) % 5 == 0:\n","    ckpt_save_path = ckpt_manager.save()\n","    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n","                                                         ckpt_save_path))\n","    # Save generated results:\n","    print(\"SAVING RESULTS\")\n","    for i, test_ls in enumerate(test_landscapes.take(10)):\n","      fc_pl_filename = f'{image_path}/reconst_{reconst_weight}_lsdisc_{ls_disc_weight}_fc_pl_epoch_{epoch+1}_{i}.png'\n","\n","      save_generated_results(generator_g, test_ls, fc_pl_filename)\n","      \n","\n","  print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n","                                                      time.time()-start))\n","                                                      "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1fi1sSQAlUMJyH9bYe-WPLWxG5UCoibRM"},"executionInfo":{"elapsed":11387,"status":"ok","timestamp":1652243691191,"user":{"displayName":"Andrew Wang","userId":"01366057211587577424"},"user_tz":240},"id":"G_36meaIoonn","outputId":"4583479a-827a-432e-a11d-fb6d5b1346f1","vscode":{"languageId":"python"}},"outputs":[],"source":["# Run the trained model on the test dataset\n","for inp in test_landscapes.take(10):\n","  generate_images(generator_g, inp)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_wWXeQ4nXkmi","vscode":{"languageId":"python"}},"outputs":[],"source":["# Reset models\n","generator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n","\n","discriminator_x = pix2pix.discriminator(norm_type='instancenorm', target=False)\n","discriminator_y = pix2pix.discriminator(norm_type='instancenorm', target=False)"]},{"cell_type":"markdown","metadata":{"id":"NwcDUT6dqg8R"},"source":["### reconst_weight=0.5, ls_disc_weight=2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":754},"id":"mOkEWtiuoofg","outputId":"ed37681e-3a5d-4929-e6ae-964c37f30dfb","vscode":{"languageId":"python"}},"outputs":[],"source":["reconst_weight=0.5\n","ls_disc_weight=2\n","\n","for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  n = 0\n","  for image_x, image_y in tf.data.Dataset.zip((train_landscapes, train_camos)):\n","    train_step(image_x, image_y, reconst_weight=reconst_weight, ls_disc_weight=ls_disc_weight)\n","    if n % 10 == 0:\n","      print ('.', end='')\n","    n += 1\n","\n","  clear_output(wait=True)\n","  # Using a consistent image (sample_horse) so that the progress of the model\n","  # is clearly visible.\n","  generate_images(generator_g, sample_landscape)\n","  generate_patched_landscape(generator_g, sample_landscape)\n","\n","\n","  if (epoch + 1) % 5 == 0:\n","    ckpt_save_path = ckpt_manager.save()\n","    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n","                                                         ckpt_save_path))\n","    # Save generated results:\n","    print(\"SAVING RESULTS\")\n","    for i, test_ls in enumerate(test_landscapes.take(10)):\n","      fc_pl_filename = f'{image_path}/reconst_{reconst_weight}_lsdisc_{ls_disc_weight}_fc_pl_epoch_{epoch+1}_{i}.png'\n","\n","      save_generated_results(generator_g, test_ls, fc_pl_filename)\n","      \n","\n","  print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n","                                                      time.time()-start))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JZgR0crmoodt","vscode":{"languageId":"python"}},"outputs":[],"source":["# Run the trained model on the test dataset\n","for inp in test_landscapes.take(10):\n","  generate_images(generator_g, inp)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dj6FP8Zdoobn","vscode":{"languageId":"python"}},"outputs":[],"source":["# Reset models\n","generator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n","\n","discriminator_x = pix2pix.discriminator(norm_type='instancenorm', target=False)\n","discriminator_y = pix2pix.discriminator(norm_type='instancenorm', target=False)"]},{"cell_type":"markdown","metadata":{"id":"iQwAu46mqj5M"},"source":["### reconst_weight=0.5, ls_disc_weight=5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tVDAn0qtooZT","vscode":{"languageId":"python"}},"outputs":[],"source":["reconst_weight=0.5\n","ls_disc_weight=5\n","\n","for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  n = 0\n","  for image_x, image_y in tf.data.Dataset.zip((train_landscapes, train_camos)):\n","    train_step(image_x, image_y, reconst_weight=reconst_weight, ls_disc_weight=ls_disc_weight)\n","    if n % 10 == 0:\n","      print ('.', end='')\n","    n += 1\n","\n","  clear_output(wait=True)\n","  # Using a consistent image (sample_horse) so that the progress of the model\n","  # is clearly visible.\n","  generate_images(generator_g, sample_landscape)\n","  generate_patched_landscape(generator_g, sample_landscape)\n","\n","\n","  if (epoch + 1) % 5 == 0:\n","    ckpt_save_path = ckpt_manager.save()\n","    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n","                                                         ckpt_save_path))\n","    # Save generated results:\n","    print(\"SAVING RESULTS\")\n","    for i, test_ls in enumerate(test_landscapes.take(10)):\n","      fc_pl_filename = f'{image_path}/reconst_{reconst_weight}_lsdisc_{ls_disc_weight}_fc_pl_epoch_{epoch+1}_{i}.png'\n","\n","      save_generated_results(generator_g, test_ls, fc_pl_filename)\n","      \n","\n","  print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n","                                                      time.time()-start))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-SnmpEu7ooVq","vscode":{"languageId":"python"}},"outputs":[],"source":["# Run the trained model on the test dataset\n","for inp in test_landscapes.take(10):\n","  generate_images(generator_g, inp)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YGD1Mh6CooS3","vscode":{"languageId":"python"}},"outputs":[],"source":["# Reset models\n","generator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n","\n","discriminator_x = pix2pix.discriminator(norm_type='instancenorm', target=False)\n","discriminator_y = pix2pix.discriminator(norm_type='instancenorm', target=False)"]},{"cell_type":"markdown","metadata":{"id":"XZ7-3IYdqqRV"},"source":["## reconst_weight=0.5, ls_disc_weight=10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cydu6twmooNh","vscode":{"languageId":"python"}},"outputs":[],"source":["reconst_weight=0.5\n","ls_disc_weight=10\n","\n","for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  n = 0\n","  for image_x, image_y in tf.data.Dataset.zip((train_landscapes, train_camos)):\n","    train_step(image_x, image_y, reconst_weight=reconst_weight, ls_disc_weight=ls_disc_weight)\n","    if n % 10 == 0:\n","      print ('.', end='')\n","    n += 1\n","\n","  clear_output(wait=True)\n","  # Using a consistent image (sample_horse) so that the progress of the model\n","  # is clearly visible.\n","  generate_images(generator_g, sample_landscape)\n","  generate_patched_landscape(generator_g, sample_landscape)\n","\n","\n","  if (epoch + 1) % 5 == 0:\n","    ckpt_save_path = ckpt_manager.save()\n","    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n","                                                         ckpt_save_path))\n","    # Save generated results:\n","    print(\"SAVING RESULTS\")\n","    for i, test_ls in enumerate(test_landscapes.take(10)):\n","      fc_pl_filename = f'{image_path}/reconst_{reconst_weight}_lsdisc_{ls_disc_weight}_fc_pl_epoch_{epoch+1}_{i}.png'\n","\n","      save_generated_results(generator_g, test_ls, fc_pl_filename)\n","      \n","\n","  print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n","                                                      time.time()-start))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tdSQ-8oiooLO","vscode":{"languageId":"python"}},"outputs":[],"source":["# Run the trained model on the test dataset\n","for inp in test_landscapes.take(10):\n","  generate_images(generator_g, inp)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l4Hz2daepX9X","vscode":{"languageId":"python"}},"outputs":[],"source":["# Reset models\n","generator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n","\n","discriminator_x = pix2pix.discriminator(norm_type='instancenorm', target=False)\n","discriminator_y = pix2pix.discriminator(norm_type='instancenorm', target=False)"]},{"cell_type":"markdown","metadata":{"id":"ZR8Cwnf3qr4t"},"source":["## reconst_weight=0, ls_disc_weight=1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"utdfdVyLpX1Y","vscode":{"languageId":"python"}},"outputs":[],"source":["reconst_weight=0\n","ls_disc_weight=1\n","\n","for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  n = 0\n","  for image_x, image_y in tf.data.Dataset.zip((train_landscapes, train_camos)):\n","    train_step(image_x, image_y, reconst_weight=reconst_weight, ls_disc_weight=ls_disc_weight)\n","    if n % 10 == 0:\n","      print ('.', end='')\n","    n += 1\n","\n","  clear_output(wait=True)\n","  # Using a consistent image (sample_horse) so that the progress of the model\n","  # is clearly visible.\n","  generate_images(generator_g, sample_landscape)\n","  generate_patched_landscape(generator_g, sample_landscape)\n","\n","\n","  if (epoch + 1) % 5 == 0:\n","    ckpt_save_path = ckpt_manager.save()\n","    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n","                                                         ckpt_save_path))\n","    # Save generated results:\n","    print(\"SAVING RESULTS\")\n","    for i, test_ls in enumerate(test_landscapes.take(10)):\n","      fc_pl_filename = f'{image_path}/reconst_{reconst_weight}_lsdisc_{ls_disc_weight}_fc_pl_epoch_{epoch+1}_{i}.png'\n","\n","      save_generated_results(generator_g, test_ls, fc_pl_filename)\n","      \n","\n","  print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n","                                                      time.time()-start))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2uWJdc38pXyJ","vscode":{"languageId":"python"}},"outputs":[],"source":["# Run the trained model on the test dataset\n","for inp in test_landscapes.take(10):\n","  generate_images(generator_g, inp)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ofxmTQnKpXuW","vscode":{"languageId":"python"}},"outputs":[],"source":["# Reset models\n","generator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n","\n","discriminator_x = pix2pix.discriminator(norm_type='instancenorm', target=False)\n","discriminator_y = pix2pix.discriminator(norm_type='instancenorm', target=False)"]},{"cell_type":"markdown","metadata":{"id":"Vr5SHJUZqt-f"},"source":["## reconst_weight=0, ls_disc_weight=2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5vWVP0iBpXr1","vscode":{"languageId":"python"}},"outputs":[],"source":["reconst_weight=0\n","ls_disc_weight=2\n","\n","for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  n = 0\n","  for image_x, image_y in tf.data.Dataset.zip((train_landscapes, train_camos)):\n","    train_step(image_x, image_y, reconst_weight=reconst_weight, ls_disc_weight=ls_disc_weight)\n","    if n % 10 == 0:\n","      print ('.', end='')\n","    n += 1\n","\n","  clear_output(wait=True)\n","  # Using a consistent image (sample_horse) so that the progress of the model\n","  # is clearly visible.\n","  generate_images(generator_g, sample_landscape)\n","  generate_patched_landscape(generator_g, sample_landscape)\n","\n","\n","  if (epoch + 1) % 5 == 0:\n","    ckpt_save_path = ckpt_manager.save()\n","    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n","                                                         ckpt_save_path))\n","    # Save generated results:\n","    print(\"SAVING RESULTS\")\n","    for i, test_ls in enumerate(test_landscapes.take(10)):\n","      fc_pl_filename = f'{image_path}/reconst_{reconst_weight}_lsdisc_{ls_disc_weight}_fc_pl_epoch_{epoch+1}_{i}.png'\n","\n","      save_generated_results(generator_g, test_ls, fc_pl_filename)\n","      \n","\n","  print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n","                                                      time.time()-start))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jBBGDFQ-pXoP","vscode":{"languageId":"python"}},"outputs":[],"source":["# Run the trained model on the test dataset\n","for inp in test_landscapes.take(10):\n","  generate_images(generator_g, inp)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DpOmPiUPpXlY","vscode":{"languageId":"python"}},"outputs":[],"source":["# Reset models\n","generator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n","\n","discriminator_x = pix2pix.discriminator(norm_type='instancenorm', target=False)\n","discriminator_y = pix2pix.discriminator(norm_type='instancenorm', target=False)"]},{"cell_type":"markdown","metadata":{"id":"LrBv1Q_BqyHD"},"source":["## reconst_weight=0, ls_disc_weight=5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-1oswddSpXcc","vscode":{"languageId":"python"}},"outputs":[],"source":["reconst_weight=0\n","ls_disc_weight=5\n","\n","for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  n = 0\n","  for image_x, image_y in tf.data.Dataset.zip((train_landscapes, train_camos)):\n","    train_step(image_x, image_y, reconst_weight=reconst_weight, ls_disc_weight=ls_disc_weight)\n","    if n % 10 == 0:\n","      print ('.', end='')\n","    n += 1\n","\n","  clear_output(wait=True)\n","  # Using a consistent image (sample_horse) so that the progress of the model\n","  # is clearly visible.\n","  generate_images(generator_g, sample_landscape)\n","  generate_patched_landscape(generator_g, sample_landscape)\n","\n","\n","  if (epoch + 1) % 5 == 0:\n","    ckpt_save_path = ckpt_manager.save()\n","    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n","                                                         ckpt_save_path))\n","    # Save generated results:\n","    print(\"SAVING RESULTS\")\n","    for i, test_ls in enumerate(test_landscapes.take(10)):\n","      fc_pl_filename = f'{image_path}/reconst_{reconst_weight}_lsdisc_{ls_disc_weight}_fc_pl_epoch_{epoch+1}_{i}.png'\n","\n","      save_generated_results(generator_g, test_ls, fc_pl_filename)\n","      \n","\n","  print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n","                                                      time.time()-start))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EI_J_eHQpXNb","vscode":{"languageId":"python"}},"outputs":[],"source":["# Run the trained model on the test dataset\n","for inp in test_landscapes.take(10):\n","  generate_images(generator_g, inp)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ECLsYQoZqE8p","vscode":{"languageId":"python"}},"outputs":[],"source":["# Reset models\n","generator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n","\n","discriminator_x = pix2pix.discriminator(norm_type='instancenorm', target=False)\n","discriminator_y = pix2pix.discriminator(norm_type='instancenorm', target=False)"]},{"cell_type":"markdown","metadata":{"id":"O6hqdWTrq0QP"},"source":["## reconst_weight=0, ls_disc_weight=10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MtvZFgsSqE4T","vscode":{"languageId":"python"}},"outputs":[],"source":["reconst_weight=0\n","ls_disc_weight=10\n","\n","for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  n = 0\n","  for image_x, image_y in tf.data.Dataset.zip((train_landscapes, train_camos)):\n","    train_step(image_x, image_y, reconst_weight=reconst_weight, ls_disc_weight=ls_disc_weight)\n","    if n % 10 == 0:\n","      print ('.', end='')\n","    n += 1\n","\n","  clear_output(wait=True)\n","  # Using a consistent image (sample_horse) so that the progress of the model\n","  # is clearly visible.\n","  generate_images(generator_g, sample_landscape)\n","  generate_patched_landscape(generator_g, sample_landscape)\n","\n","\n","  if (epoch + 1) % 5 == 0:\n","    ckpt_save_path = ckpt_manager.save()\n","    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n","                                                         ckpt_save_path))\n","    # Save generated results:\n","    print(\"SAVING RESULTS\")\n","    for i, test_ls in enumerate(test_landscapes.take(10)):\n","      fc_pl_filename = f'{image_path}/reconst_{reconst_weight}_lsdisc_{ls_disc_weight}_fc_pl_epoch_{epoch+1}_{i}.png'\n","\n","      save_generated_results(generator_g, test_ls, fc_pl_filename)\n","      \n","\n","  print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n","                                                      time.time()-start))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gxLEOXBSqE1G","vscode":{"languageId":"python"}},"outputs":[],"source":["# Run the trained model on the test dataset\n","for inp in test_landscapes.take(10):\n","  generate_images(generator_g, inp)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UapW-oowqExK","vscode":{"languageId":"python"}},"outputs":[],"source":["# Reset models\n","generator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n","\n","discriminator_x = pix2pix.discriminator(norm_type='instancenorm', target=False)\n","discriminator_y = pix2pix.discriminator(norm_type='instancenorm', target=False)"]},{"cell_type":"markdown","metadata":{"id":"O_BYocGrq2Ef"},"source":["## reconst_weight=5, ls_disc_weight=1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZHhJ2cpWqEuQ","vscode":{"languageId":"python"}},"outputs":[],"source":["reconst_weight=5\n","ls_disc_weight=1\n","\n","for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  n = 0\n","  for image_x, image_y in tf.data.Dataset.zip((train_landscapes, train_camos)):\n","    train_step(image_x, image_y, reconst_weight=reconst_weight, ls_disc_weight=ls_disc_weight)\n","    if n % 10 == 0:\n","      print ('.', end='')\n","    n += 1\n","\n","  clear_output(wait=True)\n","  # Using a consistent image (sample_horse) so that the progress of the model\n","  # is clearly visible.\n","  generate_images(generator_g, sample_landscape)\n","  generate_patched_landscape(generator_g, sample_landscape)\n","\n","\n","  if (epoch + 1) % 5 == 0:\n","    ckpt_save_path = ckpt_manager.save()\n","    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n","                                                         ckpt_save_path))\n","    # Save generated results:\n","    print(\"SAVING RESULTS\")\n","    for i, test_ls in enumerate(test_landscapes.take(10)):\n","      fc_pl_filename = f'{image_path}/reconst_{reconst_weight}_lsdisc_{ls_disc_weight}_fc_pl_epoch_{epoch+1}_{i}.png'\n","\n","      save_generated_results(generator_g, test_ls, fc_pl_filename)\n","      \n","\n","  print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n","                                                      time.time()-start))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gbXx6zChqEqO","vscode":{"languageId":"python"}},"outputs":[],"source":["# Run the trained model on the test dataset\n","for inp in test_landscapes.take(10):\n","  generate_images(generator_g, inp)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xbNkLhRNqEnO","vscode":{"languageId":"python"}},"outputs":[],"source":["# Reset models\n","generator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n","\n","discriminator_x = pix2pix.discriminator(norm_type='instancenorm', target=False)\n","discriminator_y = pix2pix.discriminator(norm_type='instancenorm', target=False)"]},{"cell_type":"markdown","metadata":{"id":"ZraAATzzq4kI"},"source":["## reconst_weight=5, ls_disc_weight=2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WpHNG5LnqEi4","vscode":{"languageId":"python"}},"outputs":[],"source":["reconst_weight=5\n","ls_disc_weight=2\n","\n","for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  n = 0\n","  for image_x, image_y in tf.data.Dataset.zip((train_landscapes, train_camos)):\n","    train_step(image_x, image_y, reconst_weight=reconst_weight, ls_disc_weight=ls_disc_weight)\n","    if n % 10 == 0:\n","      print ('.', end='')\n","    n += 1\n","\n","  clear_output(wait=True)\n","  # Using a consistent image (sample_horse) so that the progress of the model\n","  # is clearly visible.\n","  generate_images(generator_g, sample_landscape)\n","  generate_patched_landscape(generator_g, sample_landscape)\n","\n","\n","  if (epoch + 1) % 5 == 0:\n","    ckpt_save_path = ckpt_manager.save()\n","    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n","                                                         ckpt_save_path))\n","    # Save generated results:\n","    print(\"SAVING RESULTS\")\n","    for i, test_ls in enumerate(test_landscapes.take(10)):\n","      fc_pl_filename = f'{image_path}/reconst_{reconst_weight}_lsdisc_{ls_disc_weight}_fc_pl_epoch_{epoch+1}_{i}.png'\n","\n","      save_generated_results(generator_g, test_ls, fc_pl_filename)\n","      \n","\n","  print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n","                                                      time.time()-start))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"99VpWJyTqEe3","vscode":{"languageId":"python"}},"outputs":[],"source":["# Run the trained model on the test dataset\n","for inp in test_landscapes.take(10):\n","  generate_images(generator_g, inp)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t3lT7h3jqVAK","vscode":{"languageId":"python"}},"outputs":[],"source":["# Reset models\n","generator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n","\n","discriminator_x = pix2pix.discriminator(norm_type='instancenorm', target=False)\n","discriminator_y = pix2pix.discriminator(norm_type='instancenorm', target=False)"]},{"cell_type":"markdown","metadata":{"id":"O3bXL1OHq6R6"},"source":["## reconst_weight=5, ls_disc_weight=5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NOO3MttSqU9i","vscode":{"languageId":"python"}},"outputs":[],"source":["reconst_weight=5\n","ls_disc_weight=5\n","\n","for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  n = 0\n","  for image_x, image_y in tf.data.Dataset.zip((train_landscapes, train_camos)):\n","    train_step(image_x, image_y, reconst_weight=reconst_weight, ls_disc_weight=ls_disc_weight)\n","    if n % 10 == 0:\n","      print ('.', end='')\n","    n += 1\n","\n","  clear_output(wait=True)\n","  # Using a consistent image (sample_horse) so that the progress of the model\n","  # is clearly visible.\n","  generate_images(generator_g, sample_landscape)\n","  generate_patched_landscape(generator_g, sample_landscape)\n","\n","\n","  if (epoch + 1) % 5 == 0:\n","    ckpt_save_path = ckpt_manager.save()\n","    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n","                                                         ckpt_save_path))\n","    # Save generated results:\n","    print(\"SAVING RESULTS\")\n","    for i, test_ls in enumerate(test_landscapes.take(10)):\n","      fc_pl_filename = f'{image_path}/reconst_{reconst_weight}_lsdisc_{ls_disc_weight}_fc_pl_epoch_{epoch+1}_{i}.png'\n","\n","      save_generated_results(generator_g, test_ls, fc_pl_filename)\n","      \n","\n","  print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n","                                                      time.time()-start))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PkvJke7oqU7B","vscode":{"languageId":"python"}},"outputs":[],"source":["# Run the trained model on the test dataset\n","for inp in test_landscapes.take(10):\n","  generate_images(generator_g, inp)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GeXi1yh7qU4U","vscode":{"languageId":"python"}},"outputs":[],"source":["# Reset models\n","generator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n","\n","discriminator_x = pix2pix.discriminator(norm_type='instancenorm', target=False)\n","discriminator_y = pix2pix.discriminator(norm_type='instancenorm', target=False)"]},{"cell_type":"markdown","metadata":{"id":"NAbIVTncq70t"},"source":["## reconst_weight=5, ls_disc_weight=10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CrT2F7VcqU0v","vscode":{"languageId":"python"}},"outputs":[],"source":["reconst_weight=5\n","ls_disc_weight=10\n","\n","for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  n = 0\n","  for image_x, image_y in tf.data.Dataset.zip((train_landscapes, train_camos)):\n","    train_step(image_x, image_y, reconst_weight=reconst_weight, ls_disc_weight=ls_disc_weight)\n","    if n % 10 == 0:\n","      print ('.', end='')\n","    n += 1\n","\n","  clear_output(wait=True)\n","  # Using a consistent image (sample_horse) so that the progress of the model\n","  # is clearly visible.\n","  generate_images(generator_g, sample_landscape)\n","  generate_patched_landscape(generator_g, sample_landscape)\n","\n","\n","  if (epoch + 1) % 5 == 0:\n","    ckpt_save_path = ckpt_manager.save()\n","    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n","                                                         ckpt_save_path))\n","    # Save generated results:\n","    print(\"SAVING RESULTS\")\n","    for i, test_ls in enumerate(test_landscapes.take(10)):\n","      fc_pl_filename = f'{image_path}/reconst_{reconst_weight}_lsdisc_{ls_disc_weight}_fc_pl_epoch_{epoch+1}_{i}.png'\n","\n","      save_generated_results(generator_g, test_ls, fc_pl_filename)\n","      \n","\n","  print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n","                                                      time.time()-start))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OPf4Ty0HqUyH","vscode":{"languageId":"python"}},"outputs":[],"source":["# Run the trained model on the test dataset\n","for inp in test_landscapes.take(10):\n","  generate_images(generator_g, inp)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ixt-AF37qUuo","vscode":{"languageId":"python"}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tWORqvRPqUry","vscode":{"languageId":"python"}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SQ4CApRuqUnr","vscode":{"languageId":"python"}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bL5h--XPqUkP","vscode":{"languageId":"python"}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0SuolaVPqUfn","vscode":{"languageId":"python"}},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"-HhLbT1Yf9re"},"source":["# OLD Experiments:"]},{"cell_type":"markdown","metadata":{"id":"H1xShJv9f6c8"},"source":["## LAMBDA = 10"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":404},"executionInfo":{"elapsed":1496868,"status":"ok","timestamp":1651863606295,"user":{"displayName":"Andrew Wang","userId":"01366057211587577424"},"user_tz":240},"id":"2M7LmLtGEMQJ","outputId":"40e4d3f2-8bd7-4912-a5f0-ef3df0058481","vscode":{"languageId":"python"}},"outputs":[],"source":["for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  n = 0\n","  for image_x, image_y in tf.data.Dataset.zip((train_landscapes, train_camos)):\n","    train_step(image_x, image_y)\n","    if n % 10 == 0:\n","      print ('.', end='')\n","    n += 1\n","\n","  clear_output(wait=True)\n","  # Using a consistent image (sample_horse) so that the progress of the model\n","  # is clearly visible.\n","  generate_images(generator_g, sample_landscape)\n","\n","  if (epoch + 1) % 5 == 0:\n","    ckpt_save_path = ckpt_manager.save()\n","    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n","                                                         ckpt_save_path))\n","\n","  print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n","                                                      time.time()-start))"]},{"cell_type":"markdown","metadata":{"id":"1RGysMU_BZhx"},"source":["## Generate using test dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1lxeyTk_syiKMnAPdJO_cs3wS0Sq4Ieoj"},"executionInfo":{"elapsed":4313,"status":"ok","timestamp":1651863610598,"user":{"displayName":"Andrew Wang","userId":"01366057211587577424"},"user_tz":240},"id":"KUgSnmy2nqSP","outputId":"8b0aeadb-e06e-4232-af90-13bcb1cb37dd","vscode":{"languageId":"python"}},"outputs":[],"source":["# Run the trained model on the test dataset\n","for inp in test_landscapes.take(10):\n","  generate_images(generator_g, inp)"]},{"cell_type":"markdown","metadata":{"id":"GatiAR0n2IO8"},"source":["# Model with LAMBDA = 0.5"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":404},"executionInfo":{"elapsed":1496365,"status":"ok","timestamp":1651867011688,"user":{"displayName":"Andrew Wang","userId":"01366057211587577424"},"user_tz":240},"id":"CXl2AuDW2GDt","outputId":"2767c0dc-e0f9-4284-97a6-1332ab668419","vscode":{"languageId":"python"}},"outputs":[],"source":["for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  n = 0\n","  for image_x, image_y in tf.data.Dataset.zip((train_landscapes, train_camos)):\n","    train_step(image_x, image_y, reconst_weight=0.5)\n","    if n % 10 == 0:\n","      print ('.', end='')\n","    n += 1\n","\n","  clear_output(wait=True)\n","  # Using a consistent image (sample_horse) so that the progress of the model\n","  # is clearly visible.\n","  generate_images(generator_g, sample_landscape)\n","\n","  if (epoch + 1) % 5 == 0:\n","    ckpt_save_path = ckpt_manager.save()\n","    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n","                                                         ckpt_save_path))\n","\n","  print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n","                                                      time.time()-start))"]},{"cell_type":"markdown","metadata":{"id":"2PhgPu8eK5f4"},"source":["# Lambda = 0.1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":404},"executionInfo":{"elapsed":1494168,"status":"ok","timestamp":1651871558742,"user":{"displayName":"Andrew Wang","userId":"01366057211587577424"},"user_tz":240},"id":"jSzHLl2WIBEh","outputId":"4e724ff9-14dd-47e8-bc25-626640a00a20","vscode":{"languageId":"python"}},"outputs":[],"source":["for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  n = 0\n","  for image_x, image_y in tf.data.Dataset.zip((train_landscapes, train_camos)):\n","    train_step(image_x, image_y, reconst_weight=0.1)\n","    if n % 10 == 0:\n","      print ('.', end='')\n","    n += 1\n","\n","  clear_output(wait=True)\n","  # Using a consistent image (sample_horse) so that the progress of the model\n","  # is clearly visible.\n","  generate_images(generator_g, sample_landscape)\n","\n","  if (epoch + 1) % 5 == 0:\n","    ckpt_save_path = ckpt_manager.save()\n","    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n","                                                         ckpt_save_path))\n","\n","  print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n","                                                      time.time()-start))"]},{"cell_type":"markdown","metadata":{"id":"JUjH4pdBUqDM"},"source":["# Lambda = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":404},"executionInfo":{"elapsed":1460365,"status":"ok","timestamp":1651874794894,"user":{"displayName":"Andrew Wang","userId":"01366057211587577424"},"user_tz":240},"id":"nh2caB-ZUpGP","outputId":"56b1ceef-4401-4ec3-b0a9-acdf198c23a7","vscode":{"languageId":"python"}},"outputs":[],"source":["for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  n = 0\n","  for image_x, image_y in tf.data.Dataset.zip((train_landscapes, train_camos)):\n","    train_step(image_x, image_y, reconst_weight=0)\n","    if n % 10 == 0:\n","      print ('.', end='')\n","    n += 1\n","\n","  clear_output(wait=True)\n","  # Using a consistent image (sample_horse) so that the progress of the model\n","  # is clearly visible.\n","  generate_images(generator_g, sample_landscape)\n","\n","  if (epoch + 1) % 5 == 0:\n","    ckpt_save_path = ckpt_manager.save()\n","    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n","                                                         ckpt_save_path))\n","\n","  print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n","                                                      time.time()-start))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1L3qCkFFb4F_3dlO2toakYuvRTMiX6uxP"},"executionInfo":{"elapsed":6072,"status":"ok","timestamp":1651875378877,"user":{"displayName":"Andrew Wang","userId":"01366057211587577424"},"user_tz":240},"id":"D6Zr-ji3cdTv","outputId":"953355a0-1288-4133-b5a3-7d9ac4fe7606","vscode":{"languageId":"python"}},"outputs":[],"source":["# Run the trained model on the test dataset\n","for inp in test_landscapes.take(10):\n","  generate_images(generator_g, inp)"]},{"cell_type":"markdown","metadata":{"id":"aawoWNqeK8L6"},"source":["# Camo to Landscape:"]},{"cell_type":"markdown","metadata":{"id":"6ApzK5nYLKpU"},"source":["Reconst weight 10"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":404},"id":"LlaNvz_8K-Zt","outputId":"b166afd4-2941-4963-9edb-b76d97cc9766","vscode":{"languageId":"python"}},"outputs":[],"source":["for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  n = 0\n","  for image_x, image_y in tf.data.Dataset.zip((train_camos, train_landscapes)):\n","    train_step(image_x, image_y)\n","    if n % 10 == 0:\n","      print ('.', end='')\n","    n += 1\n","\n","  clear_output(wait=True)\n","  # Using a consistent image (sample_horse) so that the progress of the model\n","  # is clearly visible.\n","  generate_images(generator_g, sample_camo)\n","\n","  if (epoch + 1) % 5 == 0:\n","    ckpt_save_path = ckpt_manager.save()\n","    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n","                                                         ckpt_save_path))\n","\n","  print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n","                                                      time.time()-start))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AiJ4tCIhLbH9","vscode":{"languageId":"python"}},"outputs":[],"source":["# Run the trained model on the test dataset\n","for inp in test_camos.take(10):\n","  generate_images(generator_g, inp)"]},{"cell_type":"markdown","metadata":{"id":"QjbhMNZcLVRn"},"source":["Reconst weight 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-28h9t1ELW06","vscode":{"languageId":"python"}},"outputs":[],"source":["for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  n = 0\n","  for image_x, image_y in tf.data.Dataset.zip((train_camos, train_landscapes)):\n","    train_step(image_x, image_y, reconst_weight=1)\n","    if n % 10 == 0:\n","      print ('.', end='')\n","    n += 1\n","\n","  clear_output(wait=True)\n","  # Using a consistent image (sample_horse) so that the progress of the model\n","  # is clearly visible.\n","  generate_images(generator_g, sample_camo)\n","\n","  if (epoch + 1) % 5 == 0:\n","    ckpt_save_path = ckpt_manager.save()\n","    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n","                                                         ckpt_save_path))\n","\n","  print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n","                                                      time.time()-start))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tz-nDGKELGyL","vscode":{"languageId":"python"}},"outputs":[],"source":["# Run the trained model on the test dataset\n","for inp in test_camos.take(10):\n","  generate_images(generator_g, inp)"]},{"cell_type":"markdown","metadata":{"id":"GogrZiZALYAa"},"source":["Reconst Weight 0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":404},"executionInfo":{"elapsed":1468501,"status":"ok","timestamp":1652123502809,"user":{"displayName":"Andrew Wang","userId":"01366057211587577424"},"user_tz":240},"id":"IJun4lR2LZTe","outputId":"1d432487-c3d1-4e81-de3c-a4763eb983f7","vscode":{"languageId":"python"}},"outputs":[],"source":["for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  n = 0\n","  for image_x, image_y in tf.data.Dataset.zip((train_camos, train_landscapes)):\n","    train_step(image_x, image_y, reconst_weight=0)\n","    if n % 10 == 0:\n","      print ('.', end='')\n","    n += 1\n","\n","  clear_output(wait=True)\n","  # Using a consistent image (sample_horse) so that the progress of the model\n","  # is clearly visible.\n","  generate_images(generator_g, sample_camo)\n","\n","  if (epoch + 1) % 5 == 0:\n","    ckpt_save_path = ckpt_manager.save()\n","    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n","                                                         ckpt_save_path))\n","\n","  print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n","                                                      time.time()-start))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"12v_VYYpqzXDfnS6tGS70YdMsJbvpZfTx"},"executionInfo":{"elapsed":2576,"status":"ok","timestamp":1652123502818,"user":{"displayName":"Andrew Wang","userId":"01366057211587577424"},"user_tz":240},"id":"EFN7v52CLjta","outputId":"974a3e47-3d67-4886-e936-1fe6cf96849a","vscode":{"languageId":"python"}},"outputs":[],"source":["# Run the trained model on the test dataset\n","for inp in test_camos.take(10):\n","  generate_images(generator_g, inp)"]},{"cell_type":"markdown","metadata":{"id":"ABGiHY6fE02b"},"source":["## Next steps\n","\n","This tutorial has shown how to implement CycleGAN starting from the generator and discriminator implemented in the [Pix2Pix](https://www.tensorflow.org/tutorials/generative/pix2pix) tutorial. As a next step, you could try using a different dataset from [TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/cycle_gan). \n","\n","You could also train for a larger number of epochs to improve the results, or you could implement the modified ResNet generator used in the [paper](https://arxiv.org/abs/1703.10593) instead of the U-Net generator used here."]},{"cell_type":"markdown","metadata":{"id":"AetKI2gnbytW"},"source":["-----------------------------------"]},{"cell_type":"markdown","metadata":{"id":"ep0xRC4pbvgC"},"source":["# Experiments/Scratchwork"]},{"cell_type":"markdown","metadata":{"id":"sIMy6t-UZhnJ"},"source":["## Determining cutout location"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":797,"status":"ok","timestamp":1652175490684,"user":{"displayName":"Andrew Wang","userId":"01366057211587577424"},"user_tz":240},"id":"cTHK2zleDeZ1","outputId":"9217cb73-e1fa-46d3-dece-ffd79838b006","vscode":{"languageId":"python"}},"outputs":[],"source":["cutout_size=100\n","\n","H, W = sample_landscape[0].shape[0:2]\n","H_start = int(H//2)\n","x_start = np.random.randint(0, W - cutout_size)\n","y_start = np.random.randint(H_start, H - cutout_size)\n","\n","print(x_start, y_start)"]},{"cell_type":"markdown","metadata":{"id":"MqfxSRJZZjYH"},"source":["## Getting indices of cutout square"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":283,"status":"ok","timestamp":1652176560849,"user":{"displayName":"Andrew Wang","userId":"01366057211587577424"},"user_tz":240},"id":"1xGxdwjvWZdS","outputId":"b522c466-326e-4538-83c9-fc79dc9ec02d","vscode":{"languageId":"python"}},"outputs":[],"source":["xs = list(range(x_start, x_start+cutout_size))\n","ys = list(range(y_start, y_start+cutout_size))\n","\n","pairs = np.dstack(np.meshgrid(xs, ys)).reshape(-1, 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ec-NnAXVFJ-5","vscode":{"languageId":"python"}},"outputs":[],"source":["# top_left_x = x_start\n","# top_left_y = y_start\n","# top_left_z = 0\n","# roi_len_x = cutout_size\n","# roi_len_y = cutout_size\n","# bottom_right_z = 3\n","\n","# roi_slice = tf.slice(\n","#   sample_landscape[0],\n","#   [top_left_x, top_left_y, top_left_z],\n","#   [roi_len_x, roi_len_y, bottom_right_z]\n","# )\n","# print(roi_slice.shape)\n","# print(roi_slice)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NeylXri8FsUV","vscode":{"languageId":"python"}},"outputs":[],"source":["# roi_mask = tf.ones_like(roi_slice)\n","# mask_canvas = tf.image.pad_to_bounding_box(\n","#   [roi_mask],\n","#   top_left_x,\n","#   top_left_y,\n","#   H,\n","#   W\n","# )\n","# bool_mask = tf.cast(mask_canvas, tf.bool)\n","# print(bool_mask)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0x_oDkcZF4Nh","vscode":{"languageId":"python"}},"outputs":[],"source":["\n","\n","\n","\n","resized_camo_patch = tf.image.resize(sample_camo, [cutout_size, cutout_size])\n","\n","# # Make an editable copy of the image\n","# editable_image = tf.Variable(initial_value=sample_landscape, dtype=tf.float32)\n","# # init_op = tf.assign(editable_image, sample_landscape)\n","\n","# # Make sure we don't update the image before we've set its initial value.\n","# # with tf.control_dependencies([init_op]):\n","# update_roi.assign(editable_image, resized_camo_patch)\n","\n","\n","# # indices = tf.constant(bool_mask)\n","\n","# # update = tf.tensor_scatter_nd_update(sample_landscape, indices, resized_camo_patch)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":351,"status":"ok","timestamp":1652176669725,"user":{"displayName":"Andrew Wang","userId":"01366057211587577424"},"user_tz":240},"id":"nt0HZUp8MqfS","outputId":"479442c4-1ae3-4dcb-be63-39fe64d71031","vscode":{"languageId":"python"}},"outputs":[],"source":["# Only want x and y, from (1, 16, 16, 3)\n","# np_bool_mask = np.array(np.where(bool_mask[0,:,:,0]))\n","# print(np_bool_mask.shape)\n","\n","# list_bool_idxs = np_bool_mask.transpose()\n","list_bool_idxs = pairs\n","print(list_bool_idxs.shape)\n","\n","list_camo_pix = tf.reshape(resized_camo_patch, [cutout_size**2, 3])\n","print(list_camo_pix.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":303},"executionInfo":{"elapsed":805,"status":"ok","timestamp":1652177085432,"user":{"displayName":"Andrew Wang","userId":"01366057211587577424"},"user_tz":240},"id":"Wby0snUHB4h_","outputId":"50fe20ec-c0b6-4ab4-86b1-fb88deb66a79","vscode":{"languageId":"python"}},"outputs":[],"source":["# Ex. list of shape 4x2, want to input a channel(3)-size update\n","# indices = [[0, 0], [1, 1], [2, 2], [3, 3]]\n","# updates = tf.ones([4, 3])\n","\n","resized_camo_patch = tf.image.resize(sample_camo, [cutout_size, cutout_size])\n","indices = tf.constant(list_bool_idxs)\n","updates = tf.constant(list_camo_pix)\n","# updates = tf.zeros([cutout_size**2, 3])\n","print(resized_camo_patch.shape)\n","print(indices.shape)\n","print(updates.shape)\n","\n","update = tf.tensor_scatter_nd_update(sample_landscape[0], list_bool_idxs, list_camo_pix)\n","print(update.shape)\n","\n","update = cutout_and_replace(sample_landscape, sample_camo, cutout_size=100)\n","\n","plt.subplot(121)\n","plt.title('Landscape')\n","plt.imshow(update * 0.5 + 0.5)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"cyclegan-loss experiments.ipynb","provenance":[{"file_id":"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/cyclegan.ipynb","timestamp":1651730194224}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
